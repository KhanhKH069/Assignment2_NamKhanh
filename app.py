#app.py
import os
import warnings
from dotenv import load_dotenv
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=DeprecationWarning)

load_dotenv()

class RAGPipeline:
    def __init__(self, data_path):
        self.data_path = data_path
        self.vector_db_path = "./chroma_db"
        self.embedding_model_name = "bkai-foundation-models/vietnamese-bi-encoder"
        
        self.llm = ChatGoogleGenerativeAI(
            model="models/gemini-2.5-flash",
            temperature=0.3
        )

        print("üîÑ Loading Embeddings Model (CPU)...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name=self.embedding_model_name,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )

    def ingest_data(self):
        print(f"üìÇ Loading data from {self.data_path}...")
        loader = TextLoader(self.data_path, encoding='utf-8')
        documents = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1200,
            chunk_overlap=300,
            separators=["\n========================================\n", "\n\n", "\n", ". ", " ", ""]
        )
        chunks = text_splitter.split_documents(documents)
        print(f"üß© Split into {len(chunks)} chunks.")

        print("üíΩ Creating Vector Database...")
        vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=self.embeddings,
            persist_directory=self.vector_db_path
        )
        return vectorstore

    def get_retriever(self, vectorstore):
        retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 7}
        )
        return retriever

    def build_chain(self, retriever):
        template = """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n v·ªÅ H·ªá th·ªëng Qu·∫£n l√Ω Nghi·ªáp v·ª•. H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n t√†i li·ªáu nghi·ªáp v·ª• ƒë∆∞·ª£c cung c·∫•p.
        
        Quy tr√¨nh suy nghƒ© (Chain-of-Thought):
        1. Ph√¢n t√≠ch c√¢u h·ªèi ƒë·ªÉ hi·ªÉu √Ω ƒë·ªãnh ng∆∞·ªùi d√πng
        2. T√¨m ki·∫øm th√¥ng tin li√™n quan trong t√†i li·ªáu nghi·ªáp v·ª•
        3. T·ªïng h·ª£p th√¥ng tin v√† ƒë∆∞a ra c√¢u tr·∫£ l·ªùi ch√≠nh x√°c, chi ti·∫øt b·∫±ng ti·∫øng Vi·ªát
        4. N·∫øu c√≥ nhi·ªÅu b∆∞·ªõc ho·∫∑c ƒëi·ªÅu ki·ªán, h√£y li·ªát k√™ r√µ r√†ng
        
        L∆ØU √ù QUAN TR·ªåNG:
        - Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin c√≥ trong ng·ªØ c·∫£nh
        - N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin ‚Üí n√≥i r√µ "Th√¥ng tin n√†y kh√¥ng c√≥ trong t√†i li·ªáu"
        - Tr·∫£ l·ªùi ng·∫Øn g·ªçn, s√∫c t√≠ch nh∆∞ng ƒë·∫ßy ƒë·ªß
        - S·ª≠ d·ª•ng bullet points n·∫øu c√≥ nhi·ªÅu ƒëi·ªÉm

        Ng·ªØ c·∫£nh:
        {context}

        C√¢u h·ªèi: {question}

        Tr·∫£ l·ªùi:"""

        prompt = ChatPromptTemplate.from_template(template)

        def format_docs(docs):
            return "\n\n".join([d.page_content for d in docs])

        rag_chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | prompt
            | self.llm
            | StrOutputParser()
        )
        return rag_chain

    def run(self, query):
        if not os.path.exists(self.vector_db_path):
            print("‚ö†Ô∏è Vector DB not found. Ingesting data...")
            vectorstore = self.ingest_data()
        else:
            vectorstore = Chroma(
                persist_directory=self.vector_db_path, 
                embedding_function=self.embeddings
            )
        
        retriever = self.get_retriever(vectorstore)
        chain = self.build_chain(retriever)
        
        response = chain.invoke(query)
        return response

if __name__ == "__main__":
    input_file = "data/input.txt"
    if not os.path.exists(input_file):
        os.makedirs("data", exist_ok=True)
        with open(input_file, "w", encoding="utf-8") as f:
            f.write("H·ªá th·ªëng qu·∫£n l√Ω nghi·ªáp v·ª• - T√†i li·ªáu h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng.")
    
    pipeline = RAGPipeline(data_path=input_file)
    
    print("=" * 70)
    print("RAG PIPELINE - H·ªÜ TH·ªêNG TR·∫¢ L·ªúI C√ÇU H·ªéI NGHI·ªÜP V·ª§")
    print("=" * 70)
    
    while True:
        user_query = input("\nüìù Nh·∫≠p c√¢u h·ªèi (g√µ 'exit' ƒë·ªÉ tho√°t): ")
        if user_query.lower() in ['exit', 'quit']:
            print("\nüëã T·∫°m bi·ªát!")
            break
        
        try:
            print("\nüîç ƒêang x·ª≠ l√Ω...")
            answer = pipeline.run(user_query)
            print(f"\nü§ñ Tr·∫£ l·ªùi:\n{answer}")
        except Exception as e:
            print(f"\n‚ùå Error: {e}")